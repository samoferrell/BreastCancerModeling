---
title: "Breast Cancer Machine Learning Report"
author: "Samuel O'Ferrell"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(ggplot2)
```

### Reading in Data

```{r}
data <- read_csv("breast-cancer.csv")
#data <- data |>
#  mutate(diagnosisM = as.factor(ifelse(diagnosis == "M", 1, 0))) |>
#  select(id, diagnosis, diagnosisM, everything())
```

### Data Discussion

**Additional Information**

Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. A few of the images can be found at http://www.cs.wisc.edu/\~street/images/ Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) \[K. P. Bennett, "Decision Tree Construction Via Linear Programming." Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992\], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes. The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: \[K. P. Bennett and O. L. Mangasarian: "Robust Linear Programming Discrimination of Two Linearly Inseparable Sets", Optimization Methods and Software 1, 1992, 23-34\].

<https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic>

The original multivariate data set comes from a Wisconsin Breast Cancer study. In this study, for each breast mass, three digitized images of a fine needle aspirate were taken and analyzed. Within each image, data was collected corresonding to the cell nuclei. The characteristics studied were:

	a) radius (mean of distances from center to points on the perimeter)
	b) texture (standard deviation of gray-scale values)
	c) perimeter
	d) area
	e) smoothness (local variation in radius lengths)
	f) compactness (perimeter\^2 / area - 1.0)
	g) concavity (severity of concave portions of the contour)
	h) concave points (number of concave portions of the contour)
	i) symmetry
	j) fractal dimension ("coastline approximation" - 1)

After all this data was collected for the three images. They then found the mean, standard error, and worst recording of the three images per cell mass.

#### Variables

-   id

-   diagnosis - M for Malignant or B for Benign

-   radius_mean - Mean Radius of Lobes

-   texture_mean - Mean Standard Deviation of Gray-Scale Values of Tumor

-   perimeter_mean - Mean Outer Perimeter of Tumor

-   area_mean - Mean Area of Lobes

-   smoothness_mean - Mean Smoothness of the Tumor

-   compactness_mean - Mean Compactness of the Tumor

-   concavity_mean - Mean of Concavity of the Tumor

-   concave points_mean - Mean Number of Concave Points on the Tumor

-   symmetry_mean - Mean Symmetry of the Tumor

-   fractal_dimension_mean - Mean Fractal Dimension of Tumor

-   radius_se - SE of Radius

-   texture_se - SE of Texture

-   perimeter_se - SE of Perimeter

-   area_se - SE of Area

-   smoothness_se - SE of Smoothness

-   compactness_se - SE of Compactness

-   concavity_se - SE of Concavity

-   concave points_se - SE of Concave Points

-   symmetry_se - SE of Symmetry

-   fractal_dimension_se - SE of Fractal Dimension

-   radius_worst - Worst Radius

-   texture_worst - Worst Texture

-   perimeter_worst - Worst Perimeter

-   area_worst - Worst Area

-   smoothness_worst - Worst Smoothness

-   compactness_worst - Worst Compactness

-   concavity_worst - Worst Concavity

-   concave points_worst - Worst Concave Point

-   symmetry_worst - Worst Symmetry

-   fractal_dimension_worst - Worst Fractal Dimension

### Understanding Data

Let's look at the summary of the data:

```{r}
summary(data)
```

Checking for missing data:

```{r}
colSums(is.na(data))
```

Here we can see we have no missing data.

### Data Exploration

Now, lets make some plots to visualize the data:

```{r}
table(data$diagnosis)
```

### Modeling Testing
```{r}
library(caret)
```
```{r}
set.seed(426)
# Creating an 80/20 split
split <- createDataPartition(y = data$diagnosis, p = 0.8, list = FALSE)
train <- data[split, ]
test <- data[-split, ]
```

Logistic Model
```{r}
model <- diagnosis ~ concavity_mean + symmetry_mean + radius_mean + texture_mean
logFit <- train(log_model, 
              data = train,
              preProcess = c("center", "scale"),
              method = "glm",
              family = "binomial",
              trControl = trainControl(method = "repeatedcv", 
                                       number = 10,
                                       repeats = 3))
```

Tree Model
```{r}
treeFit <- train(model, 
              data = train,
              method = "rpart",
              preProcess = c("center", "scale"),
              trControl = trainControl(method = "repeatedcv", 
                                       number = 10,
                                       repeats = 3),
              tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.001)))
```

kNN Model
```{r}
kNNFit <- train(model, 
              data = train,
              method = "knn",
              preProcess = c("center", "scale"),
              trControl = trainControl(method = "repeatedcv", 
                                       number = 10,
                                       repeats = 3),
              tuneGrid = expand.grid(k = seq(from = 1, to = 40, by = 1)))
```

Practice Function
```{r}
modeling_function <- function(method, variables){
  variable_list <- paste(variables, collapse = " + ")
  func_model <- as.formula(paste("diagnosis ~", variable_list))
  if (method == "kNN"){
kNNFit_func <- train(func_model, 
              data = train,
              method = "knn",
              preProcess = c("center", "scale"),
              trControl = trainControl(method = "repeatedcv", 
                                       number = 10,
                                       repeats = 3),
              tuneGrid = expand.grid(k = seq(from = 1, to = 40, by = 1)))
return(kNNFit_func)
  }
  if (method == "tree"){
treeFit_func <- train(func_model, 
              data = train,
              method = "rpart",
              preProcess = c("center", "scale"),
              trControl = trainControl(method = "repeatedcv", 
                                       number = 10,
                                       repeats = 3),
              tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.001)))    
return(treeFit_func)
  }
  if (method == "logistic"){
logFit_func <- train(func_model, 
              data = train,
              preProcess = c("center", "scale"),
              method = "glm",
              family = "binomial",
              trControl = trainControl(method = "repeatedcv", 
                                       number = 10,
                                       repeats = 3))
return(logFit_func)
  }
}
```

```{r}
modeling_function(method = "kNN", variables = c("radius_mean","fractal_dimension_mean","texture_se"))
```
```{r}
variables_test = c("radius_mean","area_mean","fractal_dimension_mean")
variable_list_test <- paste(variables_test, collapse = " + ")
variable_list_test
func_model <- diagnosis ~ variable_list
```

